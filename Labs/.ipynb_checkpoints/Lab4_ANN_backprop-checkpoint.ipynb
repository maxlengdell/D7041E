{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z\n",
    "\n",
    "def ReLU(X, deriv=False):\n",
    "    res = np.maximum(0, X)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_with_err(err_str):\n",
    "    print >> sys.stderr, err_str\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "    \n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         activation=f_sigmoid))\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=ReLU))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        #exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "        \n",
    "        self.layers[-1].D = (yhat - labels).T #This is the cost function\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "            #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            #Iterates through the layers and updating weights\n",
    "            \n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "                #Eta is our learning rate and the weights are updated using gradient descent\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print (out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "\n",
    "    return chunked_data, chunked_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "\n",
    "Xtr = Xtr.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "Xtr = Xtr.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "Xtr /= 255\n",
    "X_test /= 255\n",
    "print(Xtr.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.90248 Test error: 0.90260\n",
      "[   1]  Training error: 0.90248 Test error: 0.90260\n",
      "[   2]  Training error: 0.90248 Test error: 0.90270\n",
      "[   3]  Training error: 0.90235 Test error: 0.90250\n",
      "[   4]  Training error: 0.90227 Test error: 0.90220\n",
      "[   5]  Training error: 0.90212 Test error: 0.90190\n",
      "[   6]  Training error: 0.90208 Test error: 0.90170\n",
      "[   7]  Training error: 0.90195 Test error: 0.90160\n",
      "[   8]  Training error: 0.90188 Test error: 0.90150\n",
      "[   9]  Training error: 0.90185 Test error: 0.90130\n",
      "[  10]  Training error: 0.90178 Test error: 0.90110\n",
      "[  11]  Training error: 0.90175 Test error: 0.90100\n",
      "[  12]  Training error: 0.89792 Test error: 0.89760\n",
      "[  13]  Training error: 0.88833 Test error: 0.88470\n",
      "[  14]  Training error: 0.87757 Test error: 0.87330\n",
      "[  15]  Training error: 0.86705 Test error: 0.86140\n",
      "[  16]  Training error: 0.85957 Test error: 0.85230\n",
      "[  17]  Training error: 0.85107 Test error: 0.84360\n",
      "[  18]  Training error: 0.84370 Test error: 0.83750\n",
      "[  19]  Training error: 0.83745 Test error: 0.83130\n",
      "[  20]  Training error: 0.83208 Test error: 0.82730\n",
      "[  21]  Training error: 0.82738 Test error: 0.82190\n",
      "[  22]  Training error: 0.82293 Test error: 0.81840\n",
      "[  23]  Training error: 0.81917 Test error: 0.81500\n",
      "[  24]  Training error: 0.81643 Test error: 0.81240\n",
      "[  25]  Training error: 0.81422 Test error: 0.81010\n",
      "[  26]  Training error: 0.81177 Test error: 0.80860\n",
      "[  27]  Training error: 0.81015 Test error: 0.80670\n",
      "[  28]  Training error: 0.80842 Test error: 0.80570\n",
      "[  29]  Training error: 0.80702 Test error: 0.80340\n",
      "[  30]  Training error: 0.80212 Test error: 0.80040\n",
      "[  31]  Training error: 0.79993 Test error: 0.79800\n",
      "[  32]  Training error: 0.79742 Test error: 0.79470\n",
      "[  33]  Training error: 0.79452 Test error: 0.79230\n",
      "[  34]  Training error: 0.79135 Test error: 0.78850\n",
      "[  35]  Training error: 0.78780 Test error: 0.78470\n",
      "[  36]  Training error: 0.78387 Test error: 0.78020\n",
      "[  37]  Training error: 0.77912 Test error: 0.77510\n",
      "[  38]  Training error: 0.77420 Test error: 0.77130\n",
      "[  39]  Training error: 0.76958 Test error: 0.76700\n",
      "[  40]  Training error: 0.76398 Test error: 0.76130\n",
      "[  41]  Training error: 0.75368 Test error: 0.75040\n",
      "[  42]  Training error: 0.73067 Test error: 0.72370\n",
      "[  43]  Training error: 0.55178 Test error: 0.54810\n",
      "[  44]  Training error: 0.48643 Test error: 0.47860\n",
      "[  45]  Training error: 0.34105 Test error: 0.33600\n",
      "[  46]  Training error: 0.25882 Test error: 0.25460\n",
      "[  47]  Training error: 0.16227 Test error: 0.16190\n",
      "[  48]  Training error: 0.11408 Test error: 0.11120\n",
      "[  49]  Training error: 0.10517 Test error: 0.10430\n",
      "[  50]  Training error: 0.08872 Test error: 0.08930\n",
      "[  51]  Training error: 0.07592 Test error: 0.07700\n",
      "[  52]  Training error: 0.06858 Test error: 0.07100\n",
      "[  53]  Training error: 0.06255 Test error: 0.06440\n",
      "[  54]  Training error: 0.05800 Test error: 0.06080\n",
      "[  55]  Training error: 0.05353 Test error: 0.05660\n",
      "[  56]  Training error: 0.04948 Test error: 0.05340\n",
      "[  57]  Training error: 0.04617 Test error: 0.05020\n",
      "[  58]  Training error: 0.04305 Test error: 0.04800\n",
      "[  59]  Training error: 0.04028 Test error: 0.04520\n",
      "[  60]  Training error: 0.03832 Test error: 0.04320\n",
      "[  61]  Training error: 0.03633 Test error: 0.04240\n",
      "[  62]  Training error: 0.03445 Test error: 0.04140\n",
      "[  63]  Training error: 0.03315 Test error: 0.03940\n",
      "[  64]  Training error: 0.03193 Test error: 0.03900\n",
      "[  65]  Training error: 0.03037 Test error: 0.03890\n",
      "[  66]  Training error: 0.02933 Test error: 0.03830\n",
      "[  67]  Training error: 0.02833 Test error: 0.03750\n",
      "[  68]  Training error: 0.02770 Test error: 0.03710\n",
      "[  69]  Training error: 0.02692 Test error: 0.03740\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True, eta=0.01)\n",
    "\n",
    "print(\"Done:)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "Eta = 0.05 - Classification accuracy: 97.3% \n",
    "\n",
    "Eta = 0.005 - Classification accuracy: 97.7%\n",
    "\n",
    "Eta = 0.5 - Classification accuracy: 10.3% which is about random\n",
    "\n",
    "## ReLU\n",
    "\n",
    "Eta = 0.01 - Classification accuracy: 96.2%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
